{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "rcWJeSpV5Yxi"
      },
      "source": [
        "# Further intro into PyTorch\n",
        "\n",
        "Seminar outline:\n",
        "\n",
        "- Data load\n",
        "- Automatic differentiation\n",
        "- From nn.Sequential to nn.Module\n",
        "- TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf5Ar2iU5Yxk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zlJiu--65Yxl"
      },
      "source": [
        "\n",
        "## Data load\n",
        "\n",
        "All dataset classes are subclasses of [`torch.util.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) have `__getitem__` and `__len__` methods implemented. Thus, it can be passed to a [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), which can load multiple samples parallelly.\n",
        "Popular Dataset subclasses:\n",
        "\n",
        "- [MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist)\n",
        "- [Fashion-MNIST](https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist)\n",
        "- [CIFAR](https://pytorch.org/docs/stable/torchvision/datasets.html#cifar)\n",
        "- [CelebA](https://pytorch.org/docs/stable/torchvision/datasets.html#celeba)\n",
        "\n",
        "`MNIST` constructor signature: `torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)`, where `transform` - function/transform that takes in an PIL (Height x Width x Channels) image and returns a transformed version. \n",
        "\n",
        "Several transformations can be combined together. Popular transformations:\n",
        "- `torchvision.transforms.Normalize(mean, std, inplace=False)`\n",
        "- `torchvision.transforms.ToTensor()` - Converts a PIL Image or `numpy.ndarray` (H x W x C) in the range `[0, 255]` to a `torch.FloatTensor` of shape (C x H x W) in the range `[0.0, 1.0]` \n",
        "\n",
        "\n",
        "DataLoader constructor signature\n",
        "`torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)`, where\n",
        "- dataset is DataSet instance\n",
        "- batch_size - number of items sampled at every iteration\n",
        "- num_workers - number of simultaneous reading processes (**NB** on Windows you might want to set it to `0`)\n",
        "\n",
        "DataLoaders provide convenient interface for training loops: \n",
        "\n",
        "```python\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            ...\n",
        "            \n",
        "```\n",
        "or \n",
        "```python\n",
        "    batch_X, batch_y = iter(trainloader).next()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MCPf9TX5Yxm"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "ds_train = datasets.MNIST(\".\", train=True, download=True, transform=transform)\n",
        "ds_test = datasets.MNIST(\".\", train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(ds_train, batch_size=512,\n",
        "                                        shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(ds_test, batch_size=10000,\n",
        "                                        shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4idQWvS5Yxm"
      },
      "outputs": [],
      "source": [
        "print(\"Train:\", ds_train, \"\\nTest:\", ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBqsrhBn5Yxn"
      },
      "outputs": [],
      "source": [
        "X_batch, y_batch = iter(trainloader).next()\n",
        "print(\"batch size:\", len(X_batch), \"batch dimensions:\", X_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uaabzY-5Yxn"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(4, 4), dpi=100)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.imshow(\n",
        "    torch.transpose(\n",
        "        torch.cat(\n",
        "            [X_batch[y_batch == c][:10] for c in range(10)], axis=0\n",
        "        ).reshape(10, 10, 28, 28),\n",
        "        1, 2\n",
        "    ).reshape(280, 280)\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4X4u28PX5Yxn"
      },
      "source": [
        "## Automatic differentiation\n",
        "Automatic differentiaion is the main mechanism for the backpropagation in PyTorch. PyTorch provides a module, `autograd`, for automatically calculating the gradients of tensors. We can use it to calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
        "\n",
        "You can turn off gradients for a block of code with the `torch.no_grad()` context:\n",
        "\n",
        "```\n",
        "x = torch.zeros(1, requires_grad=True)\n",
        ">>> with torch.no_grad():\n",
        "...     y = x * 2\n",
        ">>> y.requires_grad\n",
        "False\n",
        "```\n",
        "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
        "\n",
        "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K97ym-lV5Yxo"
      },
      "outputs": [],
      "source": [
        "x = torch.randn(2,2, requires_grad=True)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXKf7qsa5Yxo"
      },
      "outputs": [],
      "source": [
        "y = x**2\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3HkeLDc05Yxp"
      },
      "source": [
        "Below we can see the operation that created `y`, a power operation `PowBackward0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gixeF81u5Yxp"
      },
      "outputs": [],
      "source": [
        "## grad_fn shows the function that generated this variable\n",
        "print(y.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "cTeVBl3e5Yxp"
      },
      "source": [
        "The autograd module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtrvQd5N5Yxq"
      },
      "outputs": [],
      "source": [
        "z = y.mean()\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E4HU2Ojb5Yxq"
      },
      "source": [
        "You can check the gradients for `x` and `y` but they are empty currently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpoHkNik5Yxq"
      },
      "outputs": [],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5KVSCTFF5Yxq"
      },
      "source": [
        "To calculate the gradients, you need to run the `.backward` method on a variable `z`, for example. This will calculate the gradient for `z` with respect to `x`\n",
        "\n",
        "$$\\frac{\\partial z}{\\partial x}=\\frac{\\partial}{\\partial x}\\left[\\frac{1}{n} \\sum_{i}^{n} x_{i}^{2}\\right]=\\frac{x}{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px_Ghifq5Yxr"
      },
      "outputs": [],
      "source": [
        "z.backward()\n",
        "print(x.grad)\n",
        "print(x/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "p61O28mp5Yxr"
      },
      "source": [
        "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the loss, then, go backwards to calculate the gradients with respect to the loss. Once we have the gradients we can make a gradient descent step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jgm-_Deq5Yxr"
      },
      "source": [
        "## Loss and Autograd together\n",
        "When we create a network with PyTorch, all of the parameters are initialized with `requires_grad = True`. This means that when we calculate the loss and call `loss.backward()`, the gradients for the parameters are calculated. These gradients are used to update the weights with gradient descent. Below you can see an example of calculating the gradients using a backwards pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MQcT8EG5Yxr"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():         \n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "# create a network that stacks layers on top of each other\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 100), # add first \"dense\" layer with 784 input\n",
        "                         # units and 100 output units (hidden layer\n",
        "                         # with 100 neurons).\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10), # \"dense\" layer with 10 output\n",
        "                        # units (for 10 classes).\n",
        ").to(device)\n",
        "\n",
        "print(\"Weight shapes:\")\n",
        "for w in model.parameters():\n",
        "    print(\"  \", w.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Bb9SL-RZ5Yxs"
      },
      "source": [
        "If we give our batch to the model, we get an exception of dimension mismatch, since batch dimensions are 512 x 28 x 28 x 1, while model expects 512 x 784, so we need to flatten the trailing dimensions. You can use method `torch.flatten(input, start_dim=0, end_dim=-1) â†’ Tensor`\n",
        "\n",
        "    >>> t = torch.tensor([[[1, 2],\n",
        "                       [3, 4]],\n",
        "                      [[5, 6],\n",
        "                       [7, 8]]])\n",
        "    >>> torch.flatten(t)\n",
        "    tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "    >>> torch.flatten(t, start_dim=1)\n",
        "    tensor([[1, 2, 3, 4],\n",
        "            [5, 6, 7, 8]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d162a9c267507510531851475156d852",
          "grade": false,
          "grade_id": "cell-1c1cd227b609261d",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "fMPN6UXn5Yxt"
      },
      "outputs": [],
      "source": [
        "def flatten_trailing(batch):\n",
        "    # your code here\n",
        "    raise NotImplementedError\n",
        "    return flat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "15b617d3c055cbffdda3b13f4a25da47",
          "grade": true,
          "grade_id": "flatten",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "HA6KaPY85Yxt"
      },
      "outputs": [],
      "source": [
        "assert flatten_trailing(X_batch).shape == (512, 784)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEbBCYFV5Yxt"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "predictions = model(flatten_trailing(X_batch.to(device)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjGhs-225Yxu"
      },
      "outputs": [],
      "source": [
        "loss = loss_fn(predictions, y_batch.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WL6NO4Fk5Yxu"
      },
      "outputs": [],
      "source": [
        "print('Before backward pass: \\n', model[2].weight.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('After backward pass: \\n', model[2].weight.grad[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UZ0TZV7F5Yxu"
      },
      "source": [
        "If we try to call backward once again, the graph buffers are deallocated and we are going to get \n",
        "> RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kguw-Db65Yxu"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    loss.backward()\n",
        "except Exception as e:\n",
        "    print(\"Got Exception\", type(e), e)\n",
        "else:\n",
        "    print(\"No exception\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A76IW_7M5Yxu"
      },
      "source": [
        "## From nn.Sequential to nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XksiKI-J5Yxv"
      },
      "outputs": [],
      "source": [
        "def train(model, num_epochs=1, batch_size=512, loss_fn=loss_fn, device='cpu'):\n",
        "    # some quantities to plot\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    test_accuracy = []\n",
        "    model_dev = model.to(device)\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for i_epoch in range(num_epochs):\n",
        "        t = tqdm.tqdm(iter(trainloader), leave=False, total=len(trainloader))\n",
        "        for idx, data in enumerate(t):\n",
        "            # get the next chunk (batch) of data:\n",
        "            batch_X, batch_y = map(lambda x: x.to(device), data)\n",
        "\n",
        "            # all the black magic:\n",
        "            loss = loss_fn(model(flatten_trailing(batch_X)), batch_y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # remember the loss value at this step\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # evaluate test loss and metrics\n",
        "        test_X, test_y = map(lambda x: x.to(device), iter(testloader).next())\n",
        "\n",
        "        test_prediction = model(flatten_trailing(test_X.to(device)))\n",
        "        test_losses.append(\n",
        "            loss_fn(test_prediction, test_y).item()\n",
        "        )\n",
        "        test_accuracy.append(\n",
        "            (test_prediction.argmax(axis=1) == test_y).to(float).mean()\n",
        "        )\n",
        "\n",
        "        # all the rest is simply plotting\n",
        "\n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(8, 3), dpi=100)\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='train')\n",
        "        plt.plot(\n",
        "            np.linspace(0, len(train_losses), len(test_losses) + 1)[1:],\n",
        "            test_losses, label='test'\n",
        "        )\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xlabel(\"# steps\")\n",
        "        plt.legend();\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(test_accuracy, \"o-\")\n",
        "        plt.ylabel(\"Test accuracy\")\n",
        "        plt.xlabel(\"# epochs\");\n",
        "        plt.show()\n",
        "    return train_losses, test_losses, test_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK1GIcqc5Yxv"
      },
      "outputs": [],
      "source": [
        "train(model, num_epochs=2, device=device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0bWe7pJ5Yxw"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, n=100):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, n)\n",
        "        self.fc2 = nn.Linear(n, 10)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        Xf = X.flatten(1) # we add `flatten` here\n",
        "        X1 = F.relu(self.fc1(Xf))\n",
        "        return self.fc2(X1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0XMnND55Yxw"
      },
      "outputs": [],
      "source": [
        "model2 = Net().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YotIf8pQ5Yxw"
      },
      "outputs": [],
      "source": [
        "train(model2, device=device);"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}